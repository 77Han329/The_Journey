{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3efc9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer \n",
    "\n",
    "tokenizer = Tokenizer.from_files(vocab_path=\"vocab.json\",merges_path=\"merges.txt\",special_tokens=[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb197aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd9ea82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37efa35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3391, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class My_Embedding(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.weight =nn.Parameter(torch.empty(vocab_size,embedding_dim))\n",
    "        \n",
    "        nn.init.trunc_normal_(self.weight,mean=0,std=0.02,a=-2,b=2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.weight[x]\n",
    "    \n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "    \n",
    "embedding = Model(vocab_size=1001,embedding_dim=10)\n",
    "\n",
    "input = \"hello world\"\n",
    "\n",
    "input_ids = tokenizer.encode(input)\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "\n",
    "input = embedding(input_ids)\n",
    "\n",
    "input\n",
    "\n",
    "sim = input[0]@input[1]\n",
    "sim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b13b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "mask = torch.tril(torch.ones(3,3,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c77b67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e471939",
   "metadata": {},
   "source": [
    "# cs336 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf39803e",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bf8bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e69dbc2",
   "metadata": {},
   "source": [
    "## Implement Embedding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        factory_kwargs ={'device':device,\n",
    "                          'dtype':dtype}\n",
    "        \n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty(vocab_size,d_model,**factory_kwargs)\n",
    "        )\n",
    "        \n",
    "        std = 1\n",
    "        nn.init.trunc_normal_(self.weight, mean=0, std=std, a=-3, b=3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        return self.weight[x]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b4d5d",
   "metadata": {},
   "source": [
    "## Implement Liear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_features: int, output_features: int, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        factory_kwargs = {'device':device, 'dtype':dtype}\n",
    "        \n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty(output_features,input_features,**factory_kwargs)\n",
    "        )\n",
    "        \n",
    "        std = (2 / (input_features + output_features)) ** 0.5\n",
    "        \n",
    "        nn.init.trunc_normal_(\n",
    "            self.weight,\n",
    "            mean=0,\n",
    "            std=std,\n",
    "            a= -3 *std,\n",
    "            b= 3*std\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        ## input shape of x (B,seq,in) -> (b,seq,out)\\\n",
    "            \n",
    "        output = torch.einsum(\n",
    "            \"b s i, o i->b s o\",\n",
    "            x,self.weight\n",
    "        )\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b7d5e",
   "metadata": {},
   "source": [
    "## MHA and scaled_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82280ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMuitiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_head: int, d_model:int, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_head == 0\n",
    "        \n",
    "        self.num_head = num_head\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        self.d_head = self.d_model // self.num_head\n",
    "        \n",
    "        self.proj_q = Linear(input_features=d_model,output_features=d_model,device=device,dtype=dtype)\n",
    "        self.proj_k = Linear(input_features=d_model,output_features=d_model,device=device,dtype=dtype)\n",
    "        self.proj_v = Linear(input_features=d_model,output_features=d_model,device=device,dtype=dtype)\n",
    "        \n",
    "        self.proj_out = Linear(input_features=d_model,output_features=d_model,device=device,dtype=dtype)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        b,s,_ = x.shape\n",
    "        \n",
    "        mask = torch.tril(torch.ones(s, s, device=x.device, dtype=torch.bool))\n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)  # (1,1,s,s)  \n",
    "        q = rearrange(\n",
    "            self.proj_q(x),\n",
    "            \"b s (h d)->b h s d\",\n",
    "            h = self.num_head\n",
    "            )\n",
    "        k = rearrange(\n",
    "            self.proj_k(x),\n",
    "            \"b s (h d)->b h s d\",\n",
    "            h = self.num_head\n",
    "            )\n",
    "        v = rearrange(\n",
    "            self.proj_v(x),\n",
    "            \"b s (h d)->b h s d\",\n",
    "            h = self.num_head\n",
    "            )\n",
    "        \n",
    "        attn_score = torch.einsum(\"...nk,...mk->...nm\",q,k) / math.sqrt(self.d_head)\n",
    "        \n",
    "        attn_score = attn_score.masked_fill(\n",
    "            mask==0,\n",
    "            float('-inf')\n",
    "        )\n",
    "        \n",
    "        prob = torch.softmax(attn_score,dim=-1)\n",
    "        \n",
    "        attn_score = torch.einsum(\"...nm,...mk->...nk\",prob,v)\n",
    "        \n",
    "        attn_score = rearrange(attn_score,\"b h s d->b s (h d)\")\n",
    "        \n",
    "        return self.proj_out(attn_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
