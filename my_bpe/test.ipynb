{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 找到所有token pair 的次数\n",
    "\n",
    "\n",
    "def get_stats(tokens):\n",
    "    counts = {}\n",
    "    for pairs in zip(tokens,tokens[1:]):\n",
    "        counts[pairs] = counts.get(pairs, 0 ) + 1 \n",
    "    return counts     \n",
    "\n",
    "stats = get_stats(tokens)\n",
    "\n",
    "print(stats)\n",
    "# pair = (82,101)\n",
    "\n",
    "# decoded = ''.join(chr(b) for b in pair)\n",
    "\n",
    "# print(decoded)\n",
    "\n",
    "# sorted_dict = sorted(((v,k) for k,v in stats.items()), reverse=True)\n",
    "\n",
    "# print(sorted_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed85d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pairs = max(stats, key=stats.get)\n",
    "\n",
    "print(top_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8018b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats.get) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e6fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(tokens,pair,new_idx):\n",
    "    new_tokens = []\n",
    "    \n",
    "    i = 0 \n",
    "    \n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "            new_tokens.append(new_idx)\n",
    "            i += 2 \n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i+=1\n",
    "            \n",
    "    return new_tokens\n",
    "\n",
    "# tokens = [1,2,3,3,4,5,6,7,3,3]\n",
    "pair = (3,3)\n",
    "idx = 8\n",
    "\n",
    "new_tokens = merge(tokens,top_pairs,idx)\n",
    "\n",
    "print(new_tokens)\n",
    "print(len(new_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f63743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57cd7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset[\"train\"][\"text\"][:100]\n",
    "\n",
    "text = \"/n\".join(text)\n",
    "\n",
    "text = text.encode(\"utf-8\")\n",
    "text = list(map(int,text))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(text,vocab_size=300):\n",
    "    ## 1. encoding text to utf-8 tokens\n",
    "    ## 2. find the pair with max amount, calculate the occur times of each pairs\n",
    "    ## 3. merge 300 -256 times \n",
    "    \n",
    "    def get_token(text):\n",
    "        text = text.encode(\"utf-8\")\n",
    "        text = list(map(int,text))\n",
    "        print(len(text))\n",
    "        return text\n",
    "    \n",
    "    def get_stats(text):\n",
    "        counts = {}\n",
    "        \n",
    "        for pair in zip(text,text[1:]):\n",
    "            counts[pair] = counts.get(pair,0) + 1\n",
    "        return counts\n",
    "\n",
    "    def find_max_pair(counts):\n",
    "        pair = max(counts, key=counts.get)\n",
    "        print(pair)\n",
    "        \n",
    "        return pair\n",
    "    \n",
    "    def merge(text,merge_pair,merge_idx):\n",
    "        i = 0\n",
    "        new_stat = []\n",
    "        while i < len(text):\n",
    "            if i < len(text) -1 and text[i]==merge_pair[0] and text[i+1]==merge_pair[1]:\n",
    "                new_stat.append(merge_idx)\n",
    "                i+=2\n",
    "            else:\n",
    "                new_stat.append(text[i])\n",
    "                i+=1\n",
    "        return new_stat\n",
    "              \n",
    "                \n",
    "    text = get_token(text)\n",
    "    merge_time = vocab_size - 256\n",
    "    \n",
    "    merges = {}\n",
    "    for i in range(merge_time):\n",
    "        counts = get_stats(text)\n",
    "        max_pair = find_max_pair(counts)\n",
    "        idx = 256 + i\n",
    "        print(f\"merging {max_pair} into new index {idx}\")\n",
    "        text = merge(text, max_pair, idx)\n",
    "        merges[max_pair] = idx\n",
    "        \n",
    "    return merges\n",
    "           \n",
    "\n",
    "    \n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "dataset = dataset[\"train\"][\"text\"][:100]\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "text = \"\\n\".join(dataset)\n",
    "merges = train_bpe(text,vocab_size=400)\n",
    "\n",
    "print(merges)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    return b\"\".join(vocab[i] for i in ids).decode(\"utf-8\", errors=\"replace\")\n",
    "print(decode([122]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## decode 阶段，我们需要创建一个vocab，那就要先把原始的256 个创建进去\n",
    "\n",
    "vocab = {i : bytes([i]) for i in range(256)}\n",
    "\n",
    "\n",
    "##merges {(p0,p1):int}\n",
    "for (p0,p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "    \n",
    "def decode(input_ids):\n",
    "    \n",
    "    text = b\"\".join(vocab[i] for i in input_ids).decode(\"utf-8\", errors=\"replace\")\n",
    "    \n",
    "    return text\n",
    "def encode(text):\n",
    "    ids = []\n",
    "    \n",
    "    text.encode()\n",
    "decode([299,97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d9e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, merges):\n",
    "    \"\"\"\n",
    "    text: str\n",
    "    merges: dict[(int, int) -> int]\n",
    "    return: list[int]  (token ids)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. text -> byte tokens (0–255)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "    # 2. repeatedly apply BPE merges\n",
    "    while True:\n",
    "        # find all mergeable pairs in current tokens\n",
    "        candidates = {}\n",
    "        for pair in zip(tokens, tokens[1:]):\n",
    "            if pair in merges:\n",
    "                candidates[pair] = merges[pair]\n",
    "\n",
    "        # no mergeable pair left\n",
    "        if not candidates:\n",
    "            break\n",
    "\n",
    "        # choose the earliest learned merge\n",
    "        pair = min(candidates, key=lambda p: candidates[p])\n",
    "\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n",
    "                new_tokens.append(merges[pair])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "\n",
    "        tokens = new_tokens\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11081c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode(\"asdadhaiufaif\",merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f33663",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode([97, 115, 310, 100, 276, 105, 117, 102, 97, 105, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5bd600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "def train_bpe(\n",
    "    input_path: str,\n",
    "    vocab_size: int,\n",
    "    # special_tokens: list[str]\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \n",
    "    dataset = load_dataset(input_path)\n",
    "    raw_text = dataset[\"train\"][\"text\"][:1000]\n",
    "    text = \"\\n\".join(raw_text)\n",
    "\n",
    "    input_byte = text.encode(\"utf-8\")\n",
    "    input_ids = list(map(int,input_byte))\n",
    "    \n",
    "    ##找到inputs_ids 中每一个pair 出现的次数\n",
    "    def get_stats(input_ids):\n",
    "        stats = {}\n",
    "        \n",
    "        for pair in zip(input_ids,input_ids[1:]):\n",
    "            stats[pair] = stats.get(pair, 0) + 1\n",
    "        return stats\n",
    "    ## 找到出现最多的pair\n",
    "    def find_max_pair(stats):\n",
    "        max_pair = max(stats, key=stats.get)\n",
    "        return max_pair\n",
    "    \n",
    "    ## 更新input_ids， （把找到的最多次数的pair merge 进去然后重复）\n",
    "    def merge_ids(input_ids,merge_pair,merge_idx):\n",
    "        \n",
    "        new_ids = []\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        while i < len(input_ids):\n",
    "            \n",
    "            if i < len(input_ids) - 1 and input_ids[i] == merge_pair[0] and input_ids[i+1] == merge_pair[1]:\n",
    "                new_ids.append(merge_idx)\n",
    "                i +=2\n",
    "            else:\n",
    "                new_ids.append(input_ids[i])\n",
    "                i+=1\n",
    "                \n",
    "        return new_ids\n",
    "   \n",
    "   ## merge 我们需要的次数\n",
    "    num_merges = vocab_size - 256\n",
    "    \n",
    "    merges = {}\n",
    "\n",
    "    for i in range(num_merges):\n",
    "        stats = get_stats(input_ids)\n",
    "        max_pair = find_max_pair(stats)\n",
    "        idx = 256+i\n",
    "        input_ids = merge_ids(input_ids,merge_pair=max_pair,merge_idx=idx)\n",
    "        merges[max_pair] = idx\n",
    "        \n",
    "    ## 构造我们需要的返回的内容\n",
    "    vocab = {i:bytes([i]) for i in range(256)}\n",
    "    \n",
    "    merges_list = []\n",
    "    for (p0,p1), idx in merges.items():\n",
    "        vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        \n",
    "        merges_list.append((vocab[p0],vocab[p1]))\n",
    "    \n",
    "    return (vocab,merges_list)\n",
    "        \n",
    "vocab, merges_list = train_bpe(\"roneneldan/TinyStories\",400)\n",
    "\n",
    "print(vocab)\n",
    "print(merges_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1bc75",
   "metadata": {},
   "source": [
    "\t•\tencode：字符串 → UTF-8 bytes → 反复按 merges_list 合并 → token ids\n",
    "\t•\tdecode：token ids → vocab[id]（bytes）→ 拼起来 → UTF-8 decode\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids,vocab):\n",
    "    output = b\"\".join(vocab[i] for i in ids).decode(\"utf-8\", errors=\"replace\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "decode([72, 101, 108, 108, 111, 97, 115, 100],vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(input,vocab,merges_list):\n",
    "    ## 先把input 变成utf8\n",
    "    input_ids = [bytes([b]) for b in input.encode(\"utf-8\")]\n",
    "    ## 根据 merges list 里面的内容一点一点合并原始input ids\n",
    "    for (a,b) in merges_list:\n",
    "        \n",
    "        i = 0\n",
    "        new_ids = []\n",
    "        while i < len(input_ids):\n",
    "            if i < len(input_ids) -1 and input_ids[i] == a and input_ids[i+1] == b:\n",
    "                new_ids.append(a+b)\n",
    "                i+=2\n",
    "            else:\n",
    "                new_ids.append(input_ids[i])\n",
    "                i+=1\n",
    "        input_ids = new_ids\n",
    "    \n",
    "    ## 用vocab 变成数字\n",
    "    vocab = {v:k for k,v in vocab.items()}\n",
    "    \n",
    "    input_ids = [vocab[bytes] for bytes in input_ids]\n",
    "            \n",
    "        \n",
    "        \n",
    "    return input_ids\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.\"\n",
    "\n",
    "raw = example_text.encode(\"utf-8\")\n",
    "\n",
    "print(f\"raw text with utf8 encoding has a total lengh of {len(raw)}\")\n",
    "\n",
    "print(\"=\"*10)\n",
    "\n",
    "encoded = encode(example_text,vocab,merges_list)\n",
    "\n",
    "print(f\"raw text with trained tokenizer has a total length of {len(encoded)}\")\n",
    "\n",
    "if example_text == decode(encoded,vocab):\n",
    "    print(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee84595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe572e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(pat,\"Hello Worssrrrtttsldsre'rere!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def encode(text,pat):\n",
    "    tokens = re.findall(pat, text)\n",
    "    print(tokens)\n",
    "    ids = []\n",
    "    for tok in tokens:\n",
    "        byte_ids = list(tok.encode(\"utf-8\"))\n",
    "        ids.extend(byte_ids)\n",
    "    \n",
    "    # 再用 merges 做 BPE merge（你已经写好了逻辑）\n",
    "    # 这里略，逻辑和训练阶段一样\n",
    "    \n",
    "    return ids\n",
    "\n",
    "encode(\"Hello World Im a student!\",pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa057ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are training BPE on 500 texts on Tiny Stroies dataset\n",
      "================================================================================\n",
      "Your current training data contains 402434 chars\n",
      "Your training data contains 403999 bytes\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "input_ids = dataset[\"train\"][\"text\"][:500]\n",
    "\n",
    "print(f\"You are training BPE on {len(input_ids)} texts on Tiny Stroies dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "input_ids = \"\".join(text for text in input_ids)\n",
    "\n",
    "print(f\"Your current training data contains {len(input_ids)} chars\")\n",
    "\n",
    "input_ids = list(input_ids.encode(\"utf-8\"))\n",
    "\n",
    "len(input_ids)\n",
    "print(f\"Your training data contains {len(input_ids)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e521b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(input_path:str) -> list[int]:\n",
    "    \n",
    "    if \"TinyStories\" in input_path:\n",
    "        dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "    else: \n",
    "        raise ValueError(\"input_path need to be tiny stories!\")\n",
    "    \n",
    "    input_ids = dataset[\"train\"][\"text\"][:500]\n",
    "\n",
    "    print(f\"You are training BPE on {len(input_ids)} texts on Tiny Stroies dataset\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    input_ids = \"\".join(text for text in input_ids)\n",
    "\n",
    "    print(f\"Your current training data contains {len(input_ids)} chars\")\n",
    "\n",
    "    input_ids = list(input_ids.encode(\"utf-8\"))\n",
    "\n",
    "    len(input_ids)\n",
    "    print(f\"Your training data contains {len(input_ids)} bytes\")\n",
    "    \n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def get_stats_on_inputs_ids(input_ids:list[int]) -> dict[tuple[int,int],int]:\n",
    "    stats = {}\n",
    "    \n",
    "    for pair in zip(input_ids,input_ids[1:]):\n",
    "        stats[pair] = stats.get(pair,0) + 1 \n",
    "    \n",
    "    return stats\n",
    "\n",
    "def find_pair(stats:dict[tuple[int,int],int])->tuple[int,int]:\n",
    "    \n",
    "    pair = max(stats,key=stats.get)\n",
    "    \n",
    "    return pair\n",
    "\n",
    "def merge(input_ids:list[int], merge_pair:tuple[int,int], idx:int):\n",
    "    \n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(input_ids):\n",
    "        if i < len(input_ids) -1 and input_ids[i] == merge_pair[0] and input_ids[i+1] == merge_pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i+=2\n",
    "        else:\n",
    "            new_ids.append(input_ids[i])\n",
    "            i+=1\n",
    "    return new_ids \n",
    "\n",
    "def train_bpe(input_path: str, vocab_size: int, special_tokens: list[str]) -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \n",
    "    ## 1. preprocessing input text\n",
    "    input_ids = preprocessing(input_path=input_path)\n",
    "\n",
    "    vocab = {i:bytes([i]) for i in range(256)}\n",
    "    \n",
    "    merges = []\n",
    "     \n",
    "    num_merge = vocab_size - 256\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(num_merge):\n",
    "        current_stats = get_stats_on_inputs_ids(input_ids=input_ids)\n",
    "        merge_pair = find_pair(current_stats)\n",
    "        \n",
    "        idx = i + 256\n",
    "        \n",
    "        print(f\"merging {merge_pair} into {idx}\")\n",
    "        \n",
    "        input_ids = merge(input_ids=input_ids,\n",
    "                        merge_pair=merge_pair,\n",
    "                        idx=i+256)\n",
    "        \n",
    "        p0, p1 = vocab[merge_pair[0]], vocab[merge_pair[1]]\n",
    "        \n",
    "        merges.append((p0,p1))\n",
    "        vocab[idx] = vocab[merge_pair[0]] + vocab[merge_pair[1]]\n",
    "        \n",
    "    idx = 256 + num_merge\n",
    "    for tok in special_tokens:\n",
    "        vocab[idx] = tok.encode(\"utf-8\")\n",
    "        idx+=1\n",
    "        \n",
    "    return vocab, merges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1765254a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m         byte_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ids)\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m byte_seq\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m tok \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvocab.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerges.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(tok\u001b[38;5;241m.\u001b[39mvocab[\u001b[38;5;241m256\u001b[39m])     \u001b[38;5;66;03m# 应该是 b'th' 或类似\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tok\u001b[38;5;241m.\u001b[39mmerges))    \u001b[38;5;66;03m# 应该是 vocab_size - 256\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m, in \u001b[0;36mTokenizer.from_files\u001b[0;34m(cls, vocab_path, merges_path, special_tokens)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_files\u001b[39m(\u001b[38;5;28mcls\u001b[39m, vocab_path, merges_path, special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     16\u001b[0m     vocab \u001b[38;5;241m=\u001b[39m load_vocab(vocab_path)\n\u001b[0;32m---> 17\u001b[0m     merges \u001b[38;5;241m=\u001b[39m \u001b[43mload_merges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerges_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(vocab, merges, special_tokens)\n",
      "File \u001b[0;32m~/Desktop/The_Journey/my_bpe/train_utils.py:139\u001b[0m, in \u001b[0;36mload_merges\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m--> 139\u001b[0m         a, b \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    140\u001b[0m         merges\u001b[38;5;241m.\u001b[39mappend((a\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m\"\u001b[39m), b\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merges\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Iterator\n",
    "from train_utils import load_vocab, load_merges\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab, merges, special_tokens=None):\n",
    "        self.vocab = vocab                      # dict[int, bytes]\n",
    "        self.merges = merges                    # list[tuple[bytes, bytes]]\n",
    "        self.special_tokens = special_tokens or []\n",
    "\n",
    "        # decode 用的反向表（非常有用）\n",
    "        self.id_to_bytes = vocab\n",
    "        self.bytes_to_id = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls, vocab_path, merges_path, special_tokens=None):\n",
    "        vocab = load_vocab(vocab_path)\n",
    "        merges = load_merges(merges_path)\n",
    "        return cls(vocab, merges, special_tokens)\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        raise NotImplementedError(\"encode() not implemented yet\")\n",
    "\n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        for text in iterable:\n",
    "            yield from self.encode(text)\n",
    "\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        byte_seq = b\"\".join(self.vocab[i] for i in ids)\n",
    "        return byte_seq.decode(\"utf-8\", errors=\"replace\")\n",
    "    \n",
    "tok = Tokenizer.from_files(\"vocab.json\", \"merges.txt\")\n",
    "\n",
    "print(tok.vocab[256])     # 应该是 b'th' 或类似\n",
    "print(len(tok.merges))    # 应该是 vocab_size - 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ee8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
