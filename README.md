# The_Journey

This repository records my learning path and practice towards becoming a **Large Language Model (LLM) Algorithm Engineer**.  
It combines three parts: **LeetCode practice**, **LLM theory notes**, and **LLM algorithm implementations**.

---

## ğŸ“‚ Repository Structure
Journey2LLMengineer/
â”‚â”€â”€ README.md              # Project introduction
â”‚â”€â”€ leetcode/              # LeetCode solutions (Python/Java)
â”‚â”€â”€ llm_notes/             # LLM theory & interview notes
â”‚â”€â”€ llm_algorithms/        # Implementations of core LLM algorithms
â”‚â”€â”€ notes/                 # Paper reading and study notes

---

## ğŸ§© 1. LeetCode Solutions
- Focus on **data structures & algorithms**: array, linked list, DP, graph, etc.  
- Example:
  - `leetcode/0001_two_sum.py`
  - `leetcode/0206_reverse_linked_list.py`

---

## ğŸ“˜ 2. LLM Theory Notes
- Interview-style **Q&A** and concept explanations.  
- Topics include:
  - Transformer architecture
  - Self-Attention mechanism
  - Pre-training & Fine-tuning
  - LoRA & parameter-efficient tuning

---

## ğŸ’» 3. LLM Algorithm Implementations
- From-scratch implementations of fundamental components:
  - Attention mechanism
  - Transformer block
  - Mini GPT
  - LoRA implementation

---

## ğŸ“ 4. Paper Notes
- Reading notes for key papers:
  - BERT, GPT, LLaMA
  - Diffusion models
  - Reinforcement Learning for LLMs

---

## ğŸ¯ Goal
- Strengthen **algorithm foundation** via LeetCode  
- Build **theoretical understanding** of LLMs  
- Practice **hands-on implementation** of model components  
- Prepare for the role of **LLM Algorithm Engineer**

---

## ğŸ“… Progress
- âœ… Started repo (Oct 2025)  
- â¬œ Add first batch of LeetCode solutions  
- â¬œ Write Transformer & Attention notes  
- â¬œ Implement mini GPT model  

---

âœ¨ Stay tuned â€” this repo will grow with my learning journey!